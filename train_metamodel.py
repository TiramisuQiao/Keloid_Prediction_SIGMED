import pickle
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
import numpy as np
import pytorch_lightning as pl
import torchmetrics
import os
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping
from pytorch_lightning.loggers import TensorBoardLogger
from sklearn.preprocessing import LabelEncoder

os.makedirs('model_pth', exist_ok=True)
os.makedirs('logs', exist_ok=True)
os.makedirs('checkpoints', exist_ok=True)

class Swish(nn.Module):
    def forward(self, x):
        return x * torch.sigmoid(x)

class MetadataMLP_PL(pl.LightningModule):
    def __init__(self, input_dim=27, hidden_dim=64, output_dim=2, dropout_p=0.1, learning_rate=1e-3):
        super().__init__()
        self.save_hyperparameters()
        
        # ç‰¹å¾æå–ç½‘ç»œ
        self.feature_extractor = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            Swish(),
            nn.Dropout(dropout_p),
            
            nn.Linear(hidden_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            Swish(),
            
            nn.Linear(hidden_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            Swish()
        )
        
        # è¾“å‡ºå±‚
        self.output_layer = nn.Linear(hidden_dim, output_dim)
        
        # æŸå¤±å‡½æ•°å’ŒæŒ‡æ ‡
        self.criterion = nn.CrossEntropyLoss()
        self.train_accuracy = torchmetrics.Accuracy(task="multiclass", num_classes=output_dim)
        self.val_accuracy = torchmetrics.Accuracy(task="multiclass", num_classes=output_dim)

    def forward(self, x):
        features = self.feature_extractor(x)
        return self.output_layer(features)

    def training_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = self.criterion(logits, y)
        
        # è®°å½•æŒ‡æ ‡
        preds = torch.argmax(logits, dim=1)
        self.train_accuracy(preds, y)
        self.log('train/loss', loss, prog_bar=False)
        self.log('train/acc', self.train_accuracy, prog_bar=True)
        
        return loss

    def validation_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = self.criterion(logits, y)
        
        preds = torch.argmax(logits, dim=1)
        self.val_accuracy(preds, y)
        self.log('val/loss', loss, prog_bar=True)
        self.log('val/acc', self.val_accuracy, prog_bar=True)
        
        return loss

    def configure_optimizers(self):
        optimizer = optim.Adam(self.parameters(), lr=self.hparams.learning_rate)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3)
        return {
            'optimizer': optimizer,
            'lr_scheduler': scheduler,
            'monitor': 'val/loss'
        }
    def test_step(self, batch, batch_idx):
        inputs, labels = batch
        logits = self(inputs)  # Forward pass
        loss = self.criterion(logits, labels)  # Compute loss

        # Compute accuracy
        preds = torch.argmax(logits, dim=1)
        self.val_accuracy(preds, labels)  # å¯ä»¥å¤ç”¨ validation çš„æŒ‡æ ‡

        # Log metrics
        self.log('test_loss', loss, prog_bar=True)
        self.log('test_acc', self.val_accuracy, prog_bar=True)

        return loss

class MetadataDataset(Dataset):
    def __init__(self, meta_file, label_file):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        :param images_file: å›¾åƒæ–‡ä»¶è·¯å¾„ (.pkl)
        :param meta_file: å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„ (.pkl)
        :param label_file: æ ‡ç­¾æ–‡ä»¶è·¯å¾„ (.pkl)
        """
        # åŠ è½½æ•°æ®
        with open(meta_file, 'rb') as f:
            self.meta = pickle.load(f)
        with open(label_file, 'rb') as f:
            self.labels = pickle.load(f)

        # è½¬æ¢ä¸º NumPy æ•°ç»„
        self.meta = np.array(self.meta)
        self.labels = np.array(self.labels)


        assert  len(self.meta) == len(self.labels), "æ•°æ®é•¿åº¦ä¸ä¸€è‡´ï¼"

        # è‡ªåŠ¨ç¼–ç å­—ç¬¦ä¸²åˆ—
        if self.meta.dtype == object:
            print("ğŸ” æ£€æµ‹åˆ°å…ƒæ•°æ®åŒ…å«éæ•°å€¼ç±»å‹ï¼Œæ­£åœ¨è‡ªåŠ¨ç¼–ç ...")
            self._encode_meta()
        else:
            print("âœ”ï¸ å…ƒæ•°æ®å·²ä¸ºæ•°å€¼ç±»å‹ï¼Œè·³è¿‡ç¼–ç ")

        self.meta = self.meta.astype(np.float32)
        print(self.meta.shape)
        self.labels = self.labels.astype(np.int64)

    def _encode_meta(self):
        """
        å¯¹æ¯ä¸€åˆ—è¿›è¡Œåˆ¤æ–­ï¼Œè‹¥å«å­—ç¬¦ä¸²åˆ™ä½¿ç”¨ LabelEncoder ç¼–ç 
        """
        encoded_columns = []
        for col_idx in range(self.meta.shape[1]):
            column = self.meta[:, col_idx]
            if any(isinstance(x, str) for x in column):
                le = LabelEncoder()
                encoded_col = le.fit_transform(column).astype(float)
                encoded_columns.append(encoded_col)
            else:
                encoded_columns.append(column.astype(float))
        self.meta = np.column_stack(encoded_columns)

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        """
        è¿”å›å•ä¸ªæ ·æœ¬ (image, meta, label)ï¼Œå‡è½¬æ¢ä¸º Tensor
        """
        meta = self.meta[idx]
        label = self.labels[idx]

        # è½¬æ¢ä¸º PyTorch å¼ é‡
        meta = torch.tensor(meta, dtype=torch.float32)
        label = torch.tensor(label, dtype=torch.long)

        return meta, label
    
# --- 4. ä¸»ç¨‹åºå…¥å£ ---
if __name__ == "__main__":
    # é…ç½®å‚æ•°
    config = {
        'meta_file': "dataset/meta.pickle",
        'label_file': "dataset/label.pickle",
        'input_dim': 27,
        'hidden_dim': 64,
        'output_dim': 2,
        'dropout_p': 0.1,
        'learning_rate': 1e-3,
        'batch_size': 64,
        'max_epochs': 200,
        'train_val_split': 0.8,
        'patience': 15
    }
    
    print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–è®­ç»ƒæµç¨‹...")
    
    # åŠ è½½æ•°æ®é›†
    dataset = MetadataDataset(config['meta_file'], config['label_file'])
    if len(dataset) == 0:
        raise RuntimeError("æ•°æ®é›†åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶")

    # æ•°æ®é›†åˆ’åˆ†
    train_size = int(config['train_val_split'] * len(dataset))
    val_size = len(dataset) - train_size
    train_data, val_data = random_split(dataset, [train_size, val_size])
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(train_data, batch_size=config['batch_size'], 
                             shuffle=True, num_workers=4, pin_memory=True)
    val_loader = DataLoader(val_data, batch_size=config['batch_size'],
                           shuffle=False, num_workers=4, pin_memory=True)
    
    # åˆå§‹åŒ–æ¨¡å‹
    model = MetadataMLP_PL()
    
    # å›è°ƒå‡½æ•°
    checkpoint = ModelCheckpoint(
        dirpath='model_pth',
        filename='best_model-{epoch:02d}-{val/acc:.4f}',
        monitor='val/acc',
        mode='max',
        save_top_k=1,
        save_last=True
    )
    
    early_stop = EarlyStopping(
        monitor='val/loss',
        min_delta=0.001,
        patience=config['patience'],
        verbose=True,
        mode='min'
    )
    
    logger = TensorBoardLogger("logs", name="metadata_classifier")
    
    # è®­ç»ƒå™¨é…ç½®
    trainer = pl.Trainer(
        accelerator='auto',
        devices=1 if torch.cuda.is_available() else 'auto',
        max_epochs=config['max_epochs'],
        callbacks=[checkpoint, early_stop],
        logger=logger,
        log_every_n_steps=10,
        precision=16 if torch.cuda.is_available() else 32
    )
    
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    trainer.fit(model, train_loader, val_loader)
    
    print("ğŸ“Š æœ€ç»ˆæ¨¡å‹è¯„ä¼°:")
    result = trainer.test(model, dataloaders=val_loader)
    print(f"æœ€ç»ˆç»“æœ: {result}")
    
    print("ğŸ§  æ¨¡å‹å·²ä¿å­˜è‡³: model_pth/")